{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey97wfX02IBN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gsarti/ik-nlp-tutorials/blob/main/notebooks/W3E_BPE_Transduction.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AY-cHkhI2IBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46251426-c851-4314-dc0b-6f3c5393edf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.11.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.18.4)\n",
            "Requirement already satisfied: simplet5 in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (59.5.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: pytorch-lightning==1.5.10 in /usr/local/lib/python3.7/dist-packages (from simplet5) (1.5.10)\n",
            "Requirement already satisfied: torch!=1.8.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from simplet5) (1.10.0+cu111)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.10->simplet5) (0.18.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.10->simplet5) (2.8.0)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.10->simplet5) (0.3.1)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.10->simplet5) (0.7.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (3.3.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.5.10->simplet5) (3.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (59.5.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.11)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Run in Colab to install local packages\n",
        "!pip install spacy transformers sentencepiece tokenizers datasets simplet5\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cjz2Kam2IBP"
      },
      "source": [
        "# Training a BPE tokenizer and a Lexicon-based Transduction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb66fSRR2IBQ"
      },
      "source": [
        "*This exercise follows the explanation of using BPE tokenization as explained on Huggingface [Build a Tokenizer from Scratch](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#build-a-tokenizer-from-scratch). Adapted from a notebook by Wietse de Vries*\n",
        "\n",
        "The [Tokenizers](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) library by Huggingface provides implementations of today’s most used tokenizers (especially subword-based ones) that is both easy to use and blazing fast (Rust-compiled code!).\n",
        "\n",
        "You will start by exploring the impact of different vocabulary sizes on a subword tokenizer using the Tokenizers library, and how these can be imported and used with spaCy. Finally, you will be asked to train a small transformer model to perform transduction from feminine to masculine words.\n",
        "\n",
        "Exercise 1 is mandatory and will be part of your graded midterm portfolio. Exercise 2 is optional, but we highly recommend you to complete it, especially if you're interested in the \"Modern Neural Networks meet Linguistic Theory\" final project.\n",
        "\n",
        "## Exercise 1: Byte Pair Encoding with Huggingface Tokenizers\n",
        "\n",
        "In the following exercise, we will use a byte-pair encoding (BPE) tokenizer (see Jurafsky & Martin Sec. 2.4.3 and [Sennich et al, 2015](https://aclanthology.org/P16-1162/) to create a vocabulary of frequent words and subwords, allowing us to handle less frequent words.\n",
        "\n",
        "### Setup\n",
        "\n",
        "The following code loads a BPE tokenizer and trainer, tells the system to use whitespace as a separator and defines `[UNK]` as a special token intended to handle unknown words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GjuUwUfQ2IBQ"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\"], vocab_size=20000) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flBl1Lf-2IBQ"
      },
      "source": [
        "### Corpus\n",
        "\n",
        "The tokenizer creates a dictionary by concatenating characters and substrings into longer strings (possibly full words) based on frequency. So we need a corpus to learn what the most frequent words and substrings are. \n",
        "\n",
        "[Wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) is a dump of the (English) Wikipedia. You can use the `train_from_iterator` method to train from the data in memory, which can be done using the `wikitext` corpus in the Huggingface Datasets library. Alternatively, you can download using wget, or directly from the webpage:\n",
        "\n",
        "```shell\n",
        "wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "unzip wikitext-103-raw-v1.zip\n",
        "```\n",
        "\n",
        "The unzipped data is 500 MB. Note that the file extension for the data-files is .raw but the data is just a (unicode) text file. Because this confuses (Ubuntu) Linux, files were renamed to .raw.txt. If you maintain the original .raw filenames, adapt the path below accordingly.\n",
        "\n",
        "### Run the trainer\n",
        "\n",
        "The command below trains the tokenizer on the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LpKz5H9l2IBR",
        "outputId": "09c44949-97be-4195-d289-6fbaca3ea7dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
          ]
        }
      ],
      "source": [
        "# UNCOMMENT AND ADAPT PATH TO TRAIN ON MANUALLY DOWNLOADED DATASETS\n",
        "# data = [f'wikitext-103-raw/wiki.{split}.raw.txt' for split in ['train','test','valid']]\n",
        "# tokenizer.train(trainer, data)\n",
        "\n",
        "import datasets\n",
        "dataset = datasets.load_dataset(\n",
        "    \"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\"\n",
        ")\n",
        "\n",
        "# Build a generator to iterate over the dataset\n",
        "def batch_iterator(batch_size=1000):\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        yield dataset[i : i + batch_size][\"text\"]\n",
        "\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aHLLmZX2IBS"
      },
      "source": [
        "### Test the tokenizer\n",
        "\n",
        "Now that we have created a vocabulary, we can use it to tokenize a string into words and subtokens (for infrequent words).\n",
        "\n",
        "The example shows that most of the words are included in the vocabulary created by training on Wikipedia text, but that the acronym *UG*, the name *Hanze*, and the word *Applied*, *jointly* and *initiating* are segmented into subword strings. This suggests that these words were not seen during training, or very infrequently. (*UG* occurs 5 times in the training data and *Applied* over 200 times,  also note that the encoding is case-sensitive.). \n",
        "\n",
        "Try a few other examples to get a feeling for the lexical coverage of the tokenizer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ENQLeziA2IBS",
        "outputId": "5a9f295a-fc33-454c-c872-6c705c3c7d01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'U', 'G', 'and', 'the', 'Han', 'ze', 'University', 'of', 'Ap', 'pl', 'ied', 'Sciences', 'are', 'joint', 'ly', 'initi', 'ating', 'a', 'pilot', 'rapid', 'testing', 'centre', ',', 'which', 'will', 'start', 'on', '18', 'January', '.']\n",
            "25 words and 31 segments\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'U',\n",
              " 'G',\n",
              " 'and',\n",
              " 'the',\n",
              " 'Han',\n",
              " 'ze',\n",
              " 'University',\n",
              " 'of',\n",
              " 'Ap',\n",
              " 'pl',\n",
              " 'ied',\n",
              " 'Sciences',\n",
              " 'are',\n",
              " 'joint',\n",
              " 'ly',\n",
              " 'initi',\n",
              " 'ating',\n",
              " 'a',\n",
              " 'pilot',\n",
              " 'rapid',\n",
              " 'testing',\n",
              " 'centre',\n",
              " ',',\n",
              " 'which',\n",
              " 'will',\n",
              " 'start',\n",
              " 'on',\n",
              " '18',\n",
              " 'January',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def show_tokens(text):\n",
        "    output = tokenizer.encode(text)\n",
        "    print(f\"Tokens: {output.tokens}\")\n",
        "    number_of_words = len(tokenizer.pre_tokenizer.pre_tokenize_str(text))\n",
        "    number_of_segments = len(output.tokens)\n",
        "    print(f\"{number_of_words} words and {number_of_segments} segments\")\n",
        "\n",
        "    return output.tokens\n",
        "\n",
        "example = \"The UG and the Hanze University of Applied Sciences are jointly initiating a pilot rapid testing centre, which will start on 18 January.\"\n",
        "show_tokens(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffEjCJ3U2IBS"
      },
      "source": [
        "### Your Turn: Experiment with Vocabulary Size\n",
        "\n",
        "The training data contains 103 M tokens and has a vocabulary size of 267,000 unique types. The default setting for the trainer is to create a dictionary of max 30,000 words. This means that a fair amount of compression takes place. Even more compression can be achieved by setting the vocab_size to a smaller value. \n",
        "\n",
        "1. Choose an example text consisting of at least 100 words. You may want to ensure that it contains some rare words or tokens. \n",
        "\n",
        "2. Experiment with various settings for vocab_size.\n",
        "\n",
        "3. Count the number of words in the example, and the number of segments created by the BPE-tokenizer. Note that if the number segments goes up, more words are segmented into subwords. \n",
        "\n",
        "4. What is the vocabulary size where the number of segments is approx. 150% of the number of words? \n",
        "\n",
        "5. For this setting, what was the longest word in your example text that was not segmented? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dBAE-SmF2IBS",
        "outputId": "28077758-5838-4fc2-d1cb-e73c3e64e141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['How', 'did', 'this', 'tre', 'm', 'end', 'ous', 'development', 'take', 'place', ',', 'and', 'why', 'did', 'it', 'happ', 'en', 'in', 'the', 'Te', 'ot', 'i', 'hu', 'ac', 'án', 'Valley', '?', 'Among', 'the', 'main', 'fact', 'ors', 'are', 'Te', 'ot', 'i', 'hu', 'ac', 'án', \"'\", 's', 'ge', 'o', 'graphic', 'location', 'on', 'a', 'natural', 'trade', 'route', 'to', 'the', 'south', 'and', 'east', 'of', 'the', 'Valley', 'of', 'Mexico', ',', 'the', 'ob', 's', 'id', 'ian', 'res', 'our', 'ces', 'in', 'the', 'Te', 'ot', 'i', 'hu', 'ac', 'án', 'Valley', 'itself', ',', 'and', 'the', 'val', 'ley', \"'\", 's', 'potential', 'for', 'extensive', 'ir', 'rig', 'ation', '.', 'The', 'ex', 'act', 'role', 'of', 'other', 'fact', 'ors', 'is', 'much', 'more', 'difficult', 'to', 'p', 'in', 'point', ',', 'for', 'inst', 'ance', ',', 'Te', 'ot', 'i', 'hu', 'ac', 'án', \"'\", 's', 'religious', 'signific', 'ance', 'as', 'a', 'sh', 'r', 'ine', ',', 'the', 'historical', 'situation', 'in', 'and', 'around', 'the', 'Valley', 'of', 'Mexico', 'toward', 'the', 'end', 'of', 'the', 'first', 'mill', 'enn', 'ium', 'B', '.', 'C', '.', ',', 'the', 'ing', 'en', 'u', 'ity', 'and', 'f', 'ores', 'ight', 'ed', 'ness', 'of', 'Te', 'ot', 'i', 'hu', 'ac', 'án', \"'\", 's', 'el', 'ite', ',', 'and', ',', 'finally', ',', 'the', 'impact', 'of', 'natural', 'dis', 'ast', 'ers', ',', 'such', 'as', 'the', 'vol', 'can', 'ic', 'er', 'up', 't', 'ions', 'of', 'the', 'late', 'first', 'mill', 'enn', 'ium', 'B', '.', 'C', '.']\n",
            "143 words and 211 segments\n",
            "Longest word that is not segmented:  development\n"
          ]
        }
      ],
      "source": [
        "# TODO: Try with various vocab_sizes\n",
        "# Important: You will need to redefine the tokenizer for every new vocab size,\n",
        "# otherwise you might incur in an \"PanicError: no entry found for key\" exception\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\"],vocab_size=10000)\n",
        "\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))\n",
        "\n",
        "test_text = \"How did this tremendous development take place, and why did it happen in the Teotihuacán Valley? Among the main factors are Teotihuacán's geographic location on a natural trade route to the south and east of the Valley of Mexico, the obsidian resources in the Teotihuacán Valley itself, and the valley's potential for extensive irrigation. The exact role of other factors is much more difficult to pinpoint, for instance, Teotihuacán's religious significance as a shrine, the historical situation in and around the Valley of Mexico toward the end of the first millennium B.C., the ingenuity and foresightedness of Teotihuacán's elite, and, finally, the impact of natural disasters, such as the volcanic eruptions of the late first millennium B.C.\"\n",
        "\n",
        "tokens = show_tokens(test_text)\n",
        "\n",
        "token_dict = dict()\n",
        "\n",
        "# Answer question 5 by going over the output, or write a \n",
        "# few lines of code to provide the answer.\n",
        "\n",
        "for token in tokens:\n",
        "  token_dict[token]=len(token)\n",
        "\n",
        "longest = max(zip(token_dict.values(), token_dict.keys()))[1]\n",
        "\n",
        "print(\"Longest word that is not segmented: \", longest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Number of words = 143, number of segments = 177 for vocabulary size = 30000\n",
        "* Number of words = 143, number of segments = 630 for vocabulary size = 5000\n",
        "* Number of words = 143, number of segments = 156 for vocabulary size = 100000\n",
        "* Number of words = 143, number of segments = 189 for vocabulary size = 20000\n",
        "* Number of words = 143, number of segments = 211 for vocabulary size = 10000\n",
        "* Number of words = 143, number of segments = 223 for vocabulary size = 9000\n",
        "\n",
        "Experimenting with different vocabulary sizes, I obtained number of segments approximately 150% of number of words (number of words = 143, number of segments = 211) for vocabulary size = 10000.\n",
        "\n",
        "For the setting specified above, longest word in the text that was not segmented is \"development\".\n"
      ],
      "metadata": {
        "id": "rS0A0Fusk9hs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8E_xUz62IBT"
      },
      "source": [
        "### Loading the BPE Tokenizer into spaCy\n",
        "\n",
        "Now that you experimented with the creation of many tokenizers using Huggingface Tokenizers, you might want to move them to a more familiar environment. The following class lets you load a Huggingface Tokenizer into spaCy: the `get_words_spaces` function is used to preserve the whitespaces before tokens that are not word pieces.\n",
        "\n",
        "### Your Turn: Fill in the missing code\n",
        "\n",
        "Your task is to complete the `__call__` method of the `BPETokenizer` class to go from text to spaCy `Docs`, and finally to print the tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ztncC7PT2IBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e371c91d-ed82-423f-ee19-c3ecf3f09d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of tokens: ['Jeff', 'Be', 'z', 'os', 'is', 'a', 'billion', 'a', 'ire', 'who', 'became', 'famous', 'after', 'the', 'Dutch', 'bridge', 'controver', 'sy', '.']\n"
          ]
        }
      ],
      "source": [
        "from spacy.tokens import Doc\n",
        "from spacy.vocab import Vocab\n",
        "import spacy\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, tokenizer, vocab):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "    \n",
        "    def get_words_spaces(self, tokens):\n",
        "        words = []\n",
        "        spaces = []\n",
        "        for i, (text, (_, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):\n",
        "            words.append(text)\n",
        "            if i < len(tokens.tokens) - 1:\n",
        "                # If next start != current end we assume a \n",
        "                # space in between\n",
        "                next_start, _ = tokens.offsets[i + 1]\n",
        "                spaces.append(next_start > end)\n",
        "            else:\n",
        "                spaces.append(True)\n",
        "        return words, spaces\n",
        "\n",
        "    def __call__(self, text):\n",
        "        # TODO: Encode the texts to obtain tokens\n",
        "        tokens = self.tokenizer.encode(text)\n",
        "        # TODO: Use get_words_spaces to obtain the words and spaces\n",
        "        words, spaces = self.get_words_spaces(tokens)\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.vocab = Vocab(strings=[\n",
        "    tok for tok in tokenizer.get_vocab().keys()\n",
        "])\n",
        "nlp.tokenizer = BPETokenizer(tokenizer, nlp.vocab)\n",
        "\n",
        "text = \"Jeff Bezos is a billionaire who became famous after the Dutch bridge controversy.\"\n",
        "\n",
        "# TODO: Convert the text in a list of tokens and print them\n",
        "doc = nlp(text)\n",
        "print(\"List of tokens:\", [token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrsTwM6-2IBT"
      },
      "source": [
        "## (Optional) Exercise 2: Lexicon-based Transduction System\n",
        "\n",
        "In this exercise you will build a rule-based tool to transduce a given input text **from masculine to feminine**. You are provided with a list of pairs including feminine words and their masculine counterparts. To create a rule based transducer, the following components will be needed:\n",
        "\n",
        "1. Extract a subset of sentences from the `wikitext-103-raw-v1` containing masculine words (words from the list, gendered pronouns (e.g. he/his/him)). **Tip**: you can try to use the spaCy lemmas annotations to avoid removing inflected forms of words.\n",
        "\n",
        "Fill the `is_masculine` function so that only sentences containing masculine words are preserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwcGFisV2IBT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import datasets\n",
        "\n",
        "gender_lexicon = [\n",
        "    (\"Brother\", \"Sister\"),\n",
        "    (\"Drake\", \"Duck\"),\n",
        "    (\"Father\", \"Mother\"),\n",
        "    (\"Gentleman\", \"Lady\"),\n",
        "    (\"Husband\", \"Wife\"),\n",
        "    (\"Man\", \"Woman\"),\n",
        "    (\"Nephew\", \"Niece\"),\n",
        "    (\"Son\", \"Daughter\"),\n",
        "    (\"Wizard\", \"Witch\"),\n",
        "    (\"Boy\", \"Girl\"),\n",
        "    (\"Bull\", \"Cow\"),\n",
        "    (\"Cock\", \"Hen\"),\n",
        "    (\"Dog\", \"Bitch\"),\n",
        "    (\"Drone\", \"Bee\"),\n",
        "    (\"Gander\", \"Goose\"),\n",
        "    (\"Horse\", \"Mare\"),\n",
        "    (\"King\", \"Queen\"),\n",
        "    (\"Monk\", \"Nun\"),\n",
        "    (\"Sir\", \"Madam\"),\n",
        "    (\"Stag\", \"Hind\"),\n",
        "    (\"Stallion\", \"Mare\"),\n",
        "    (\"Tutor\", \"Governess\"),\n",
        "    (\"Drone\", \"Bee\"),\n",
        "    (\"Brother-in-law\", \"Sister-in-law\"),\n",
        "    (\"Son-in-law\", \"Daughter-in-law\"),\n",
        "    (\"Maternal-uncle\", \"Maternal-aunt\"),\n",
        "    (\"Step-son\", \"Step-daughter\"),\n",
        "    (\"Hostess\", \"Steward\"),\n",
        "    (\"Widow\", \"Widower\"),\n",
        "    (\"author\", \"authoress\"),\n",
        "    (\"count\", \"countess\"),\n",
        "    (\"heir\", \"heiress\"),\n",
        "    (\"manager\", \"manageress\"),\n",
        "    (\"patron\", \"patroness\"),\n",
        "    (\"priest\", \"priestess\"),\n",
        "    (\"baron\", \"baroness\"),\n",
        "    (\"giant\", \"giantess\"),\n",
        "    (\"host\", \"hostess\"),\n",
        "    (\"lion\", \"lioness\"),\n",
        "    (\"mayor\", \"mayoress\"),\n",
        "    (\"poet\", \"poetess\"),\n",
        "    (\"shepherd\", \"shepherdess\"),\n",
        "    (\"actor\", \"actress\"),\n",
        "    (\"conductor\", \"conductress\"),\n",
        "    (\"hunter\", \"huntress\"),\n",
        "    (\"prince\", \"princess\"),\n",
        "    (\"traitor\", \"traitress\"),\n",
        "    (\"master\", \"mistress\"),\n",
        "    (\"benefactor\", \"benefactress\"),\n",
        "    (\"founder\", \"foundress\"),\n",
        "    (\"instructor\", \"instructress\"),\n",
        "    (\"emperor\", \"empress\"),\n",
        "    (\"tiger\", \"tigress\"),\n",
        "    (\"waiter\", \"waitress\"),\n",
        "    (\"murderer\", \"murderess\"),\n",
        "    (\"hero\", \"heroine\"),\n",
        "    (\"fox\", \"vixen\"),\n",
        "    (\"sultan\", \"sultana\"),\n",
        "    (\"grandfather\", \"grandmother\"),\n",
        "    (\"manservant\", \"maidservant\"),\n",
        "    (\"milkman\", \"milkwoman\"),\n",
        "    (\"salesman\", \"saleswoman\"),\n",
        "    (\"great-uncle\", \"great-aunt\"),\n",
        "    (\"landlord\", \"landlady\"),\n",
        "    (\"he\", \"she\"),\n",
        "    (\"him\", \"her\"),\n",
        "    (\"his\", \"her\")\n",
        "]\n",
        "\n",
        "def is_masculine(text):\n",
        "    # TODO: Fill your regex with words from the wordlist\n",
        "    # (use '|'.join(...) to join them in the regex)\n",
        "    regex = None\n",
        "    return bool(re.search(regex, text, re.IGNORECASE))\n",
        "\n",
        "\n",
        "dataset = datasets.load_dataset(\n",
        "    \"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\"\n",
        ")\n",
        "\n",
        "# We consider only the first 200 characters to avoid long paragraphs\n",
        "filtered_dataset = dataset.filter(lambda x: is_masculine(x[\"text\"][:200]))\n",
        "filtered_dataset = filtered_dataset.map(lambda x: {\"text\": x[\"text\"][:200]})\n",
        "filtered_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD9GABBp2IBT"
      },
      "source": [
        "2. Create a `feminize` function that takes a sententence from the the filtered dataset and returns a feminized version of it, based on lexical pairs. Use it to create a new field \"feminine_text\" in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KwfrUlw2IBU"
      },
      "outputs": [],
      "source": [
        "def feminize(text):\n",
        "    \"\"\"Returns a feminized version of text\"\"\"\n",
        "    feminized_text = text\n",
        "    for m, f in gender_lexicon:\n",
        "        # TODO: fill in your regex to select word m (adapted from is_masculine)\n",
        "        match_regex = None \n",
        "        # TODO: fill in your regex to replace m by f\n",
        "        substitute_regex = None\n",
        "        feminized_text = re.sub(match_regex, substitute_regex, feminized_text, re.IGNORECASE)\n",
        "    return feminized_text\n",
        "\n",
        "# TODO: Use filtered_dataset.map to add a feminized version of the text column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNNi5vdY2IBU"
      },
      "source": [
        "3. Rename the `text` field to `source_text` and the `feminine_text` field to `target_text` (this is needed for `SimpleT5` to work properly). Transform the dataset to Pandas DataFrame format and use the following code to train a simple neural transduction model.\n",
        "\n",
        "*(More info on the [T5 model](https://huggingface.co/t5-small) and the [SimpleT5](https://github.com/Shivanandroy/simpleT5) library)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJj0pCVh2IBU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from simplet5 import SimpleT5\n",
        "\n",
        "# TODO: Convert the Huggingface Dataset in a Pandas dataframe and split it in training\n",
        "# and evaluation sets (you decide the sizes based on your computational resources)\n",
        "train_df, eval_df = None, None\n",
        "\n",
        "model = SimpleT5()\n",
        "model.from_pretrained(model_type=\"t5\", model_name=\"t5-small\")\n",
        "model.train(\n",
        "    train_df=train_df,\n",
        "    eval_df=eval_df, \n",
        "    source_max_token_len=128, \n",
        "    target_max_token_len=128, \n",
        "    batch_size=8, max_epochs=3, use_gpu=torch.cuda.is_available()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeyGdqXW2IBU"
      },
      "source": [
        "4. Conclude by testing the model on a few examples of your choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4uk6bIf2IBU"
      },
      "outputs": [],
      "source": [
        "model.load_model(\"t5\", \"<YOUR SAVED MODEL PATH>\", use_gpu=torch.cuda.is_available())\n",
        "\n",
        "text_to_feminize = \"my brother thought that his uncle was a duke\"\n",
        "model.predict(text_to_feminize)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f9f85d98b63f393548f3009c8d52d8286e609a1467b1184fe464fb700873fbd3"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "W3E_BPE_Transduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}